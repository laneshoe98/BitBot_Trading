{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SETUP AND IMPORTS ==========\n",
    "import pandas as pd\n",
    "import os\n",
    "import yfinance as yf\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging to record errors to a file\n",
    "logging.basicConfig(filename='data_processing_errors.log', level=logging.ERROR, \n",
    "                    format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "# Debugging mode - set to True for debugging, DEBUG_LEVEL controls verbosity\n",
    "DEBUG = True\n",
    "DEBUG_LEVEL = 2  # Level 1: Basic; Level 2: Detailed\n",
    "\n",
    "# Load environment variables for server credentials\n",
    "load_dotenv(r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\programs\\server_credentials.env')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "\n",
    "# Database engine for PostgreSQL connection\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "# Paths for Inputs\n",
    "incoming_data_paths = [\n",
    "    r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\data\\old data\\Accounts_History_2021.csv',\n",
    "    r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\data\\old data\\Accounts_History_2022.csv',\n",
    "    r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\data\\old data\\Accounts_History_2023.csv',\n",
    "    r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\data\\old data\\Accounts_History_2024.csv'\n",
    "]\n",
    "input_master_data = r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\programs\\master_data14.csv'\n",
    "\n",
    "# Paths for Outputs\n",
    "cleaned_ledger_data_output_path = r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\data\\old data'  # Fixed output folder\n",
    "    date_string = datetime.now().strftime(\"%Y%m%d\") # Date string for output file, BELOW: Variable output file name\n",
    "    consolidated_data_output_path = os.path.join(cleaned_ledger_data_output_path, f\"cleaned_ledger_data_{date_string}.csv\")\n",
    "report_symbol_verification = r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\data\\old data\\report-symbol_verification.csv'\n",
    "output_master_data69 = r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\programs\\master_data69.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DATA CLEANING FUNCTION ==========\n",
    "def clean_fidelity_data(incoming_data_paths, cleaned_ledger_data_output_path):\n",
    "    \"\"\"\n",
    "    Cleans multiple Fidelity data files, combines them into one consolidated file,\n",
    "    and saves it as cleaned_ledger_data_YYYYMMDD.csv.\n",
    "    \"\"\"\n",
    "    \n",
    "    combined_data = [\n",
    "        pd.read_csv(file_path, on_bad_lines='skip').apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "        for file_path in incoming_data_paths\n",
    "    ]\n",
    "\n",
    "    consolidated_data = pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "    consolidated_data = consolidated_data.rename(\n",
    "        columns={\n",
    "            'Run Date': 'transaction_date',\n",
    "            'Account': 'portfolio_name',\n",
    "            'Action': 'notes',\n",
    "            'Symbol': 'symbol',\n",
    "            'Description': 'asset_name',\n",
    "            'Quantity': 'quantity',\n",
    "            'Price': 'price',\n",
    "            'Amount': 'transaction_amount',\n",
    "            'Commission': 'commission',\n",
    "            'Fees': 'fees'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    consolidated_data['commission'] = consolidated_data['commission'].fillna(0)\n",
    "    consolidated_data['fees'] = consolidated_data['fees'].fillna(0)\n",
    "\n",
    "    consolidated_data.drop(columns=['Type', 'Exchange Quantity', 'Exchange Currency', 'Currency', 'Exchange Rate',\n",
    "                               'Accrued Interest', 'Settlement Date'], inplace=True, errors='ignore')\n",
    "\n",
    "    final_columns_order = ['symbol', 'asset_name', 'quantity', 'price', 'transaction_amount',\n",
    "                           'commission', 'fees', 'portfolio_name', 'transaction_date', 'notes']\n",
    "    consolidated_data = consolidated_data[final_columns_order]\n",
    "\n",
    "    consolidated_data['transaction_date'] = pd.to_datetime(consolidated_data['transaction_date'], format='%m/%d/%Y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    consolidated_data['symbol'] = consolidated_data['symbol'].astype(str).str.replace('-', '', n=1)\n",
    "\n",
    "    # Save the consolidated data using the pre-defined output path\n",
    "    consolidated_data.to_csv(consolidated_data_output_path, index=False)\n",
    "    print(\"Cleaned Ledger data saved. Please review the data for errors.\")\n",
    "\n",
    "    # Gate: Prompt user to review the cleaned data  \n",
    "    review_data = input(\"Have you reviewed the cleaned data for errors? (y/n): \").strip().lower()  \n",
    "    if review_data == 'y':  \n",
    "        print(\"Program moving on to update master_data69\") \n",
    "        return consolidated_data  \n",
    "    else:  \n",
    "        print(\"Data Processing Concluded\")  \n",
    "        exit()  # Terminate the program  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MASTER DATA69 UPDATE FUNCTION ==========\n",
    "def update_master_data(cleaned_ledger_data, input_master_data):\n",
    "    \"\"\"Adds symbols from cleaned data to master data, ensuring only valid symbols are added.\"\"\"\n",
    "    try:\n",
    "        # Load existing master data if it exists, or create an empty DataFrame\n",
    "        if os.path.exists(input_master_data):\n",
    "            master_data = pd.read_csv(input_master_data)\n",
    "        else:\n",
    "            master_data = pd.DataFrame(columns=['symbol'])\n",
    "\n",
    "        # Set of existing symbols in the master data for quick comparison\n",
    "        master_symbols_set = set(master_data['symbol'].str.upper())  # Standardized to uppercase for comparison\n",
    "\n",
    "        # Regular expression to filter out non-standard symbols\n",
    "        stock_symbol_pattern = re.compile(r'^[A-Z]{1,5}$')  # Matches 1-5 uppercase letters\n",
    "\n",
    "        # Extract and filter symbols from the cleaned data\n",
    "        cleaned_symbols = cleaned_ledger_data['symbol'].str.upper()  # Convert to uppercase for case-insensitive comparison\n",
    "        valid_symbols = cleaned_symbols[cleaned_symbols.str.match(stock_symbol_pattern)]\n",
    "        new_symbols_set = set(valid_symbols)\n",
    "\n",
    "        # Identify unique new symbols that are not already in master data\n",
    "        unique_new_symbols = new_symbols_set - master_symbols_set\n",
    "\n",
    "        # Create a DataFrame for the new symbols\n",
    "        new_entries = pd.DataFrame({'symbol': list(unique_new_symbols)})\n",
    "        \n",
    "        # Concatenate the new entries with the master data\n",
    "        updated_data = pd.concat([master_data, new_entries], ignore_index=True)\n",
    "\n",
    "        # Save the updated master data to the fixed output file for master_data69\n",
    "        updated_data.to_csv(output_master_data69, index=False)  # Use the fixed output path\n",
    "        print(f\"Master data updated and saved as: {output_master_data69}\")\n",
    "\n",
    "        return updated_data  # Return the DataFrame for the next step\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during master data update: {e}\")\n",
    "        if DEBUG and DEBUG_LEVEL >= 2:\n",
    "            print(f\"[DEBUG] Error during master data update: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ENRICH MASTER DATA WITH YFINANCE ==========\n",
    "def enrich_master_data(updated_master_data):\n",
    "    \"\"\"\n",
    "    Enriches the master data with additional information from YFinance, \n",
    "    handling cases with missing data and filling empty columns.\n",
    "    \"\"\"\n",
    "\n",
    "    for idx, symbol in enumerate(tqdm(master_data['symbol'].unique(), desc=\"Enriching data\", unit=\"symbol\")):\n",
    "        try:\n",
    "            stock = yf.Ticker(symbol)\n",
    "            info = stock.info\n",
    "\n",
    "            longname = info.get('longName', 'Unknown')\n",
    "            sector = info.get('sector', 'Unknown')\n",
    "            industry = info.get('industry', 'Unknown')\n",
    "            history = stock.history(period=\"max\")\n",
    "            first_traded = history.index.min().strftime('%Y-%m-%d') if not history.empty else 'Unknown'\n",
    "\n",
    "            master_data.loc[master_data['symbol'] == symbol, ['longname', 'sector', 'industry', 'first_traded']] = [\n",
    "                longname, sector, industry, first_traded\n",
    "            ]\n",
    "\n",
    "            if DEBUG and DEBUG_LEVEL >= 2 and idx % 50 == 0:\n",
    "                print(f\"[DEBUG] Enriched symbol {symbol}: longname={longname}, sector={sector}, industry={industry}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error enriching symbol {symbol}: {e}\")\n",
    "            if DEBUG and DEBUG_LEVEL >= 2:\n",
    "                print(f\"[DEBUG] Error enriching symbol {symbol}: {e}\")\n",
    "            master_data.loc[master_data['symbol'] == symbol, ['longname', 'sector', 'industry', 'first_traded']] = [\n",
    "                'Unknown', 'Unknown', 'Unknown', 'Unknown'\n",
    "            ]\n",
    "\n",
    "    # Fill missing values in other columns\n",
    "    master_data['asset_name'] = master_data['asset_name'].fillna('Unknown')\n",
    "    master_data['industry'] = master_data['industry'].fillna('Unknown')\n",
    "    master_data['first_traded'] = master_data['first_traded'].fillna('Unknown')\n",
    "\n",
    "    return master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DATABASE UPLOAD WITH USER PROMPT ==========\n",
    "def upload_to_database(data):\n",
    "    \"\"\"Prompts the user to confirm if they want to upload the enriched master data to PostgreSQL.\"\"\"\n",
    "    user_input = input(\"Do you want to upload the data to PostgreSQL? (y/n): \").strip().lower()\n",
    "    if user_input == 'y':\n",
    "        try:\n",
    "            data.to_sql('asset_ledger', con=engine, if_exists='append', index=False)\n",
    "            print(\"Data successfully inserted into the database.\")\n",
    "            print(\"Program run successfully, new master_data file created, uploaded to PostgreSQL.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during database upload: {e}\")\n",
    "            if DEBUG and DEBUG_LEVEL >= 2:\n",
    "                print(f\"[DEBUG] Error during database upload: {e}\")\n",
    "    else:\n",
    "        print(\"Program run successfully, new master_data file created, not uploaded to PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MAIN SCRIPT EXECUTION ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Clean the new data files\n",
    "    cleaned_ledger_data = clean_fidelity_data(incoming_data_paths, cleaned_ledger_data_output_path)\n",
    "    \n",
    "    if cleaned_ledger_data is not None: \n",
    "        if DEBUG and DEBUG_LEVEL >= 1:\n",
    "            print(f\"[DEBUG] Total cleaned files processed: {len(cleaned_ledger_data)}\")  \n",
    "\n",
    "        # Step 2: Update master data with new symbols from the cleaned data\n",
    "        updated_master_data = update_master_data(cleaned_ledger_data, input_master_data)  # Get the updated DataFrame\n",
    "        \n",
    "        if updated_master_data is not None:\n",
    "            # Save the updated master data with a new version\n",
    "            new_input_master_data = increment_filename_version(input_master_data)\n",
    "            updated_master_data.to_csv(new_input_master_data, index=False)\n",
    "            print(f\"New master data created and saved as: {new_input_master_data}\")\n",
    "\n",
    "            # Prompt user to confirm before proceeding with enrichment and database upload\n",
    "            proceed = input(\"Do you want to proceed with enrichment and database upload? (y/n): \").strip().lower()\n",
    "            if proceed != 'y':\n",
    "                print(\"Process terminated by user after master data generation. No enrichment or upload performed.\")\n",
    "                exit()  # Exit the program if user does not wish to proceed\n",
    "\n",
    "            # Step 3: Enrich the updated master data\n",
    "            enriched_master_data = enrich_master_data(updated_master_data)  # Use the updated DataFrame\n",
    "            enriched_master_data.to_csv(new_input_master_data, index=False)  # Overwrite with enriched data\n",
    "            print(f\"Enriched master data saved to: {new_input_master_data}\")\n",
    "            \n",
    "            # Step 4: Prompt for database upload\n",
    "            upload_to_database(enriched_master_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
