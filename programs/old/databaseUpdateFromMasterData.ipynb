{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries and Set Up Database Connection\n",
    "# Purpose: Load necessary libraries, environment variables, and establish a connection to the PostgreSQL database.\n",
    "\n",
    "import pandas as pd  # For data handling\n",
    "import json  # To format JSON fields\n",
    "from sqlalchemy import create_engine, text  # For database operations\n",
    "from dotenv import load_dotenv  # To load environment variables\n",
    "import os  # For interacting with the operating system\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\programs\\server_credentials.env')\n",
    "\n",
    "# Retrieve database credentials from environment variables\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "\n",
    "# Establish a connection to the PostgreSQL database\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "print(\"Database connection established.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the master_data CSV and Format Columns\n",
    "# Purpose: Load the CSV file and format 'sector' and 'industry' columns as JSON arrays for database compatibility.\n",
    "\n",
    "# Load master_data CSV file into a DataFrame\n",
    "file_path = r'C:\\Users\\Lane\\Documents\\Projects\\trading_bot\\data\\master_data12.csv'\n",
    "new_data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'sector' and 'industry' columns to JSON arrays (with one element each) for database compatibility\n",
    "new_data['sector'] = new_data['sector'].apply(lambda x: json.dumps([x]) if pd.notna(x) else json.dumps([]))\n",
    "new_data['industry'] = new_data['industry'].apply(lambda x: json.dumps([x]) if pd.notna(x) else json.dumps([]))\n",
    "\n",
    "print(\"Data loaded and formatted from master_data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New records successfully inserted into tracked_companies.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load Existing Data from Database\n",
    "# Purpose: Load existing data from the tracked_companies table to compare it with new_data and identify updates or new entries.\n",
    "\n",
    "# Load data from the database table to compare with new_data\n",
    "with engine.connect() as connection:\n",
    "    existing_data = pd.read_sql(\n",
    "        'SELECT symbol, asset_name, sector, industry, first_traded, index_inclusion FROM tracked_companies', \n",
    "        connection\n",
    "    )\n",
    "print(\"Existing data loaded from the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Identify and Insert New Records\n",
    "# Purpose: Insert any completely new symbols from master_data that aren't in the database.\n",
    "#          This step ensures that any new stocks or companies in master_data get added.\n",
    "\n",
    "# Identify records in new_data that are not already in the database\n",
    "new_records = new_data[~new_data['symbol'].isin(existing_data['symbol'])]\n",
    "\n",
    "# Insert these new records into the database\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        new_records.to_sql('tracked_companies', con=connection, if_exists='append', index=False)\n",
    "    print(\"New symbols and data successfully inserted into tracked_companies.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during new record insertion: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lane\\AppData\\Local\\Temp\\ipykernel_16780\\572457725.py:13: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if pd.isna(row[col]) or row[col] == '' or row[col] == '[]':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CCJ with values: {'index_inclusion': '2016/12/01', 'symbol': 'CCJ'}\n",
      "Existing records updated where needed.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Identify Incomplete Records in Database\n",
    "# Purpose: Identify records in the database that already exist (i.e., symbols already in tracked_companies)\n",
    "#          but have NULL values in specific fields. These records need additional data from master_data.\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    incomplete_records = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT symbol, asset_name, sector, industry, first_traded, index_inclusion \n",
    "        FROM tracked_companies \n",
    "        WHERE asset_name IS NULL OR sector IS NULL OR industry IS NULL OR first_traded IS NULL OR index_inclusion IS NULL\n",
    "        \"\"\",\n",
    "        connection\n",
    "    )\n",
    "print(\"Incomplete symbols identified for updating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Update Incomplete Records with Data from master_data\n",
    "# Purpose: Update missing fields for symbols in the database that already exist but have incomplete data.\n",
    "#          Uses master_data to fill in only the missing fields, keeping the database as accurate as possible.\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        for _, row in incomplete_records.iterrows():\n",
    "            symbol = row['symbol']\n",
    "            update_values = {}\n",
    "\n",
    "            # Find the corresponding data in master_data for the symbol\n",
    "            new_info = new_data[new_data['symbol'] == symbol]\n",
    "            if not new_info.empty:\n",
    "                # Check each field, and only add to update_values if the field in the database is null\n",
    "                for col in ['asset_name', 'sector', 'industry', 'first_traded', 'index_inclusion']:\n",
    "                    if pd.isna(row[col]) or row[col] == '' or row[col] == '[]':\n",
    "                        if pd.notna(new_info.iloc[0][col]) and new_info.iloc[0][col] != '':\n",
    "                            update_values[col] = new_info.iloc[0][col]\n",
    "\n",
    "                # Execute the update statement if there are fields to update\n",
    "                if update_values:\n",
    "                    update_values['symbol'] = symbol  # Include symbol for WHERE clause\n",
    "                    update_stmt = f\"\"\"\n",
    "                    UPDATE tracked_companies\n",
    "                    SET {', '.join([f\"{col} = :{col}\" for col in update_values.keys() if col != 'symbol'])}\n",
    "                    WHERE symbol = :symbol\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    # Execute the update with update_values as a dictionary\n",
    "                    connection.execute(text(update_stmt), update_values)\n",
    "                    print(f\"Updated {symbol} with values: {update_values}\")\n",
    "\n",
    "    print(\"Existing symbols updated successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during record updates: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Verification (Optional)\n",
    "# Purpose: Verify updates by reloading data from the tracked_companies table to ensure that updates were applied correctly.\n",
    "#          This is an optional step, helpful for validating the update process.\n",
    "\n",
    "# Verify updates by reloading data from the tracked_companies table\n",
    "with engine.connect() as connection:\n",
    "    verified_data = pd.read_sql(\n",
    "        'SELECT symbol, asset_name, sector, industry, first_traded, index_inclusion FROM tracked_companies', \n",
    "        connection\n",
    "    )\n",
    "print(\"Data verification complete. Updated data loaded for review.\")\n",
    "verified_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
